# F1 Analytics

[üá∫üá∏ Read in English](README_en.md)

Projeto para explorar, consolidar e visualizar dados historicos da Formula 1 usando Python, SQLite e Jupyter Notebooks.
Feito por Jo√£o Marcolin (com um pouco de assist√™ncia de AI).

## Visao geral do fluxo

1.  **Download dos dados (Manual)** - Os dados completos (1950-2025+) s√£o baixados manualmente do *dump* do banco de dados Jolpica e colocados em `data/raw/`.
2.  **Criacao do banco** - `src/data_processing/create_db.py` carrega os CSVs do *dump* em um banco SQLite localizado em `data/processed/f1.db`.
3.  **Consultas e analises** - SQLs em `data/db_queries/` e utilitarios Python (`src/modules/data_processing/db_reader.py`, `src/analysis/data_viz/plotter.py` etc) suportam notebooks exploratorios e geracao de graficos.

**NOTA SOBRE DADOS ANTIGOS:** O m√©todo anterior de download via `src/data_processing/kaggle_download.py` (usando `kagglehub`) √© agora considerado **obsoleto**. Ele ainda funciona, mas os dados do Kaggle param em 2024 e n√£o s√£o mais a fonte principal deste projeto.

## Pr√©-requisitos

-   Python 3.12+
-   (Opcional) Ambiente virtual para isolar dependencias.

## Configuracao rapida

```bash
# 1. Criar e ativar um ambiente virtual (opcional, mas recomendado)
python -m venv .venv
# Windows PowerShell
.venv\Scripts\Activate.ps1
# No Linux/macOS, use: source .venv/bin/activate

# 2. Instalar depend√™ncias
pip install -r requirements.txt
````

**3) Fazer download e preparar os dados (Processo Manual)**

Como o download autom√°tico (`download_data.py`) pode ser inst√°vel, o m√©todo recomendado √© o manual:

1.  **Baixe o .zip:** No seu navegador, acesse a URL do *dump* de dados do Jolpica:
    `https://api.jolpi.ca/data/dumps/download/delayed/?dump_type=csv`
2.  **Mova e Descompacte:** Mova o arquivo `.zip` baixado para a pasta `data/raw/` do projeto e descompacte-o l√°.
3.  **Limpe (Opcional):** Voc√™ pode apagar o arquivo `.zip` ap√≥s a extra√ß√£o.

**4) Criar o Banco de Dados**

Ap√≥s os arquivos CSV estarem em `data/raw/`, execute o script de cria√ß√£o do banco:

```bash
python src/data_processing/create_db.py
```

Apos esses passos, o banco `data/processed/f1.db` estara pronto para uso nas consultas e notebooks.

### 5) Gerar a Feature Store (Pipeline de Features)

Para alimentar modelos de ML e an√°lises avan√ßadas, o projeto conta com um pipeline que processa dados brutos e cria tabelas de features consolidadas (ex.: ritmo de corrida, confiabilidade, performance).

Para gerar as features, execute:

```bash
python src/data_processing/feature_pipeline.py
```

Isso criar√° os seguintes arquivos em `data/features/`:
- `pace_features.csv`: M√©tricas de ritmo de corrida (consist√™ncia, compara√ß√£o com companheiro, ritmo de classifica√ß√£o).
- `performance_features.csv`: M√©tricas de resultados (pontos, posi√ß√µes ganhas, duelos de classifica√ß√£o).
- `reliability_features.csv`: Taxas de quebra (DNF) e falhas mec√¢nicas.
- `experience_features.csv`: M√©tricas de experi√™ncia (n√∫mero de corridas, p√≥dios, pole positions, etc.).

## Ferramentas do projeto

  - `src/modules/db_reader.py`: classe `DbReader` para executar queries SQL retornando `pandas.DataFrame`.
  - `data/db_queries/*.sql`: consultas prontas (resultados de corridas, classificatorias, tempos de volta etc.) para alimentar analises.
  - `notebooks/utils.py`: funcoes auxiliares para analise temporal, filtros por piloto/circuito e geracao de graficos especificos.
  - `src/data_viz/plotter.py`: classe `Plotter` com atalhos para graficos recorrentes em analises exploratorias.
  - `src/features/` - componentes reutilizaveis para cria√ß√£o de features e vari√°veis utilizadas em an√°lises.
    - Aqui a l√≥gica √© de Domain-Driven Design (DDD). Separo em fam√≠lias de features e cada uma recebe seus datasets e l√≥gicas espec√≠ficas.

### Exemplo rapido de uso do `DbReader`

```python
from src.modules.db_reader import DbReader

reader = DbReader()  # usa data/processed/f1.db por padrao
df_results = reader.run_query_file("data/db_queries/race_results_report.sql")
print(df_results.head())
```

## Estrutura de pastas

  - `data/raw/` - CSVs originais baixados do *dump* do Jolpica.
  - `data/processed/` - artefatos prontos para consumo (SQLite, vistas, relatorios).
  - `data/db_queries/` - consultas SQL reutilizaveis.
  - `notebooks/` - analises exploratorias (`explore_data.ipynb`, `analise_verstappen.ipynb` etc.).
  - `src/data_processing/` - scripts de ingestao e preparacao.
  - `src/data_viz/` - utilitarios para visualizacao.
  - `src/modules/` - componentes reutilizaveis (ex.: `DbReader`).
  - `src/modules/features/` - l√≥gica de extra√ß√£o de features (Pace, Reliability, Performance).
  - `data/features/` - sa√≠da do pipeline de features (arquivos CSV prontos para uso).

## Notebooks em destaque

  - `notebooks/explore_data.ipynb` - exploracao inicial das tabelas consolidadas.
  - `notebooks/analise_verstappen.ipynb` - estudo focado no desempenho de Max Verstappen (usa funcoes de `notebooks/utils.py`).
  - `notebooks/2025_championship` - estudo focado em uma retrospectiva e an√°lise do campeonato de 2025.
